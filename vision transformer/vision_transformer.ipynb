{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_id = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(cuda_id)\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8a100858fc8b50b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download and preparethe data\n",
    "\n",
    "We'll use the CIFAR10 dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f51fdad69ce1e7a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86e29a36b84c2248"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 100"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8b0e5f6d01c396"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download and augment the training dataset\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "data = CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(size=32, ratio=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(0, 1),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(0, 1),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "test_loader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ddb240309e17171"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = data.classes\n",
    "classes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2cc10088ac10168"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print out an example image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2277894ab71e829"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b0b70f16ea958b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display(image, size=8):\n",
    "    print(image[0].shape)\n",
    "    image = image[0].permute((1, 2, 0))\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6403a2595f0aea8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(data[14])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaed33a246824db1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split a batch of data into batch of patches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "465cffcc2714f724"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "369ba5d45a1b4be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_patch(tensor, step=4):\n",
    "    if len(tensor.shape) == 3:\n",
    "        return (\n",
    "            tensor.unfold(0, 3, 3)\n",
    "            .unfold(1, step, step)\n",
    "            .unfold(2, step, step)\n",
    "            .flatten(start_dim=0, end_dim=2)\n",
    "        )\n",
    "    return (\n",
    "        tensor.unfold(1, 3, 3)\n",
    "        .unfold(2, step, step)\n",
    "        .unfold(3, step, step)\n",
    "        .squeeze()\n",
    "        .flatten(start_dim=1, end_dim=2)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc9a852d8ac38345"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = next(iter(data_loader))[0]\n",
    "\n",
    "step = 4\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(\n",
    "    (\n",
    "        vutils.make_grid(\n",
    "            batch_patch(d, step)[0], nrow=32 // step, padding=1, normalize=True\n",
    "        )\n",
    "        .permute(1, 2, 0)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d34b5fd5ea80488"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implement the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e62f272b295fbb17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4367f5eb69f307e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First implement the learnable positional encoding layer - it was used in DDPM lab"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f785d80b1d212b0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, input_size=65, d=25, n=10000, output_size=256):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        denominator = torch.pow(n, torch.arange(d) / d)\n",
    "        numerator = torch.arange(input_size).reshape(-1, 1)\n",
    "        self.inputs = torch.concat(\n",
    "            [torch.sin(numerator / denominator), torch.cos(numerator / denominator)], 1\n",
    "        ).to(device)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2 * d, out_features=output_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=output_size, out_features=output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, k):\n",
    "        return self.network(self.inputs[k])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1b0a49648cf48b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement a transformer block - We'll need a few of them so it's easier to make a separate class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3349943544c2b8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_dims=256, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(n_dims)\n",
    "        self.attention = nn.MultiheadAttention(n_dims, n_heads)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.LayerNorm(n_dims),\n",
    "            nn.Linear(n_dims, 2 * n_dims),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2 * n_dims, n_dims),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        post_norm = self.norm(input)\n",
    "        post_attention = self.attention(post_norm, post_norm, post_norm)[0] + input\n",
    "        return self.network(post_attention) + post_attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0c0b2525e1a83cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whole model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cd89879c2ced40b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VisionTransformerModel(nn.Module):\n",
    "    def __init__(self, n_classes, patch_size=4, im_size=32, n_dims=256):\n",
    "        super(VisionTransformerModel, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.token = nn.Parameter(\n",
    "            torch.zeros(256, dtype=torch.float32), requires_grad=True\n",
    "        ).to(device)\n",
    "        self.positional_embedding_keys = torch.arange((im_size // patch_size) ** 2 + 1)\n",
    "        self.patch_flatten = nn.Flatten(start_dim=-3)\n",
    "        self.embed = nn.Linear(3 * patch_size**2, n_dims)\n",
    "        self.positional_encode = PositionalEncoding(input_size=n_dims, output_size=256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.transformers = nn.Sequential(\n",
    "            TransformerBlock(n_dims=n_dims),\n",
    "            TransformerBlock(n_dims=n_dims),\n",
    "            TransformerBlock(n_dims=n_dims),\n",
    "            TransformerBlock(n_dims=n_dims),\n",
    "            TransformerBlock(n_dims=n_dims),\n",
    "            TransformerBlock(n_dims=n_dims),\n",
    "        ).to(device)\n",
    "        self.normalization = nn.LayerNorm(n_dims)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(256, 2 * n_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2 * n_dims, n_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        patched = batch_patch(img, self.patch_size)\n",
    "        flattened = self.patch_flatten(patched)\n",
    "        embedded = self.embed(flattened)\n",
    "\n",
    "        class_token = torch.ones(img.shape[0]).to(device).reshape(-1, 1, 1) * self.token\n",
    "        with_token = torch.concat([class_token, embedded], 1)\n",
    "\n",
    "        with_encode = with_token + self.positional_encode(\n",
    "            self.positional_embedding_keys\n",
    "        )\n",
    "\n",
    "        tr = self.dropout1(with_encode)\n",
    "        tr = self.transformers(tr)[:, 0]\n",
    "        normalized = self.normalization(tr)\n",
    "        return self.MLP(normalized)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42d2eb72a01a9dd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare the model for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3379bcd0be342b6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = VisionTransformerModel(len(data.classes)).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9991d03b14aacff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for p in model.modules(): print(p)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a6e20c09c207772"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100, 150], 0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59467a0864c1d81c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = nn.functional.one_hot(\n",
    "    torch.tensor([i for i in range(len(data.classes))]), num_classes=len(classes)\n",
    ").detach()\n",
    "labels = torch.as_tensor(labels,dtype=torch.float32,device=device)\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "352c376ed5933b8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(160):\n",
    "    iteration = 0\n",
    "    total_loss = 0\n",
    "    print(\"epoch:\", epoch)\n",
    "    for batch in data_loader:\n",
    "        iteration = 0\n",
    "        total_loss = 0\n",
    "        batch_data = batch[0].to(device)\n",
    "        batch_labels = labels[batch[1]]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_data)\n",
    "        loss = loss_fn(pred, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        iteration += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\" - loss:\", total_loss.item() / iteration)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb530baee34620f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9680434b2f874c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(100 * correct / total, \"%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "785a6df7cf23bddb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = next(iter(test_loader))[0][:16].to(device)\n",
    "\n",
    "classification = torch.argmax(model(d), 1)\n",
    "\n",
    "for c in classification:\n",
    "    print(classes[c])\n",
    "\n",
    "step = 4\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(\n",
    "    (\n",
    "        vutils.make_grid(d, nrow=4, padding=1, normalize=True)\n",
    "        .permute(1, 2, 0)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fd2ac6a5ab13d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ee75a18f87960a2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
